{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "df = pd.read_csv('../data/Wholesale customers data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver informacion del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(df.info())\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does each column mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = {\n",
    "    'Channel': 'Canal de distribución (1=HORECA/Hotel/Restaurant/Café, 2=Retail)',\n",
    "    'Region': 'Región geográfica (1-3)',\n",
    "    'Fresh': 'Productos frescos/perecederos (gasto anual en unidades monetarias)',\n",
    "    'Milk': 'Productos lácteos (gasto anual en unidades monetarias)',\n",
    "    'Grocery': 'Productos de alimentación (gasto anual en unidades monetarias)',\n",
    "    'Frozen': 'Productos congelados (gasto anual en unidades monetarias)',\n",
    "    'Detergents_Paper': 'Detergentes y productos de papel (gasto anual en unidades monetarias)',\n",
    "    'Delicassen': 'Productos delicatessen (gasto anual en unidades monetarias)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any categorical data to convert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variables categóricas:\")\n",
    "print(\"\\nChannel valores únicos:\", df['Channel'].unique())\n",
    "print(\"Region valores únicos:\", df['Region'].unique())\n",
    "\n",
    "# Sí, tenemos dos variables categóricas: Channel y Region\n",
    "# Para clustering, deberíamos convertirlas usando one-hot encoding si queremos incluirlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any missing data to remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValores faltantes por columna:\")\n",
    "print(df.isnull().sum())\n",
    "# No hay valores faltantes en el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column collinearity - any high correlations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()\n",
    "\n",
    "# Alta correlación entre:\n",
    "# - Grocery y Detergents_Paper (0.92)\n",
    "# - Milk y Grocery (0.73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics - any outliers to remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar boxplots para detectar outliers\n",
    "plt.figure(figsize=(15, 6))\n",
    "df.boxplot(column=['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Boxplots para detectar outliers')\n",
    "plt.show()\n",
    "\n",
    "# Calcular IQR para cada columna numérica\n",
    "for column in df.columns[2:]:\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    print(f\"\\n{column}:\")\n",
    "    print(f\"Número de outliers: {len(df[(df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column-wise data distribution - is the distribution skewed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns[2:]):\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribución de {col}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular skewness\n",
    "print(\"\\nAsimetría (skewness) por columna:\")\n",
    "print(df.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareto principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el principio de Pareto\n",
    "for column in df.columns[2:]:\n",
    "    sorted_values = np.sort(df[column])[::-1]\n",
    "    cumsum = np.cumsum(sorted_values)\n",
    "    percentile_20 = np.percentile(np.arange(len(sorted_values)), 20)\n",
    "    percentage_80 = (cumsum[int(percentile_20)] / cumsum[-1]) * 100\n",
    "    print(f\"{column}: El 20% superior representa el {percentage_80:.2f}% del total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- ex.: Frozen, Grocery, Milk and Detergents Paper have a high...\n",
    "- ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear una copia del dataframe original\n",
    "clean_customers = df.copy()\n",
    "\n",
    "# 2. Tratar los outliers usando el método IQR\n",
    "def remove_outliers(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[column] >= lower_bound) & (df_clean[column] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "# 3. Aplicar la limpieza a las columnas numéricas\n",
    "numeric_columns = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "clean_customers = remove_outliers(clean_customers, numeric_columns)\n",
    "\n",
    "# 4. Verificar el tamaño del dataset después de la limpieza\n",
    "print(\"Tamaño original del dataset:\", len(df))\n",
    "print(\"Tamaño después de remover outliers:\", len(clean_customers))\n",
    "\n",
    "# 5. Convertir variables categóricas usando one-hot encoding\n",
    "clean_customers = pd.get_dummies(clean_customers, columns=['Channel', 'Region'], prefix=['Channel', 'Region'])\n",
    "\n",
    "# 6. Verificar el nuevo dataset\n",
    "print(\"\\nColumnas en el dataset limpio:\")\n",
    "print(clean_customers.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "-  La eliminación de outliers puede reducir significativamente el tamaño del dataset\n",
    "-  Las transformaciones realizadas nos preparan para el proceso de clustering\n",
    "-  Mantenemos todas las variables ya que cada una puede aportar información valiosa para identificar patrones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importar StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2. Crear el scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 3. Ajustar y transformar los datos numéricos\n",
    "# Seleccionamos solo las columnas numéricas (excluyendo las dummy variables creadas)\n",
    "numeric_columns = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "customers_scale = clean_customers.copy()\n",
    "\n",
    "# 4. Aplicar el escalado\n",
    "customers_scale[numeric_columns] = scaler.fit_transform(customers_scale[numeric_columns])\n",
    "\n",
    "# 5. Verificar los resultados\n",
    "print(\"Estadísticas descriptivas después del escalado:\")\n",
    "print(customers_scale[numeric_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calcular SSE para diferentes valores de k\n",
    "sse = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(customers_scale)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Visualizar el método del codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, sse, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method para Número Óptimo de Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2).fit(customers_scale)\n",
    "\n",
    "labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_customers['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el número de clientes en cada cluster\n",
    "print(\"\\nDistribución de clientes por cluster:\")\n",
    "print(clean_customers['Label'].value_counts())\n",
    "\n",
    "# Analizar las características de cada cluster\n",
    "print(\"\\nMedias por cluster:\")\n",
    "print(clean_customers.groupby('Label')[numeric_columns].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Aplicar DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels_dbscan = dbscan.fit_predict(customers_scale)\n",
    "\n",
    "# Añadir las etiquetas al dataframe\n",
    "clean_customers['Label_DBSCAN'] = labels_dbscan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el número de clusters y puntos de ruido\n",
    "print(\"\\nDistribución de clientes por cluster (DBSCAN):\")\n",
    "print(clean_customers['Label_DBSCAN'].value_counts())\n",
    "\n",
    "# Analizar las características de cada cluster\n",
    "print(\"\\nMedias por cluster (DBSCAN):\")\n",
    "print(clean_customers.groupby('Label_DBSCAN')[numeric_columns].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, hue, ax):\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue,\n",
    "                    ax=ax)\n",
    "    ax.set_title(f'{x} vs {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Detergents_Paper vs Milk\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# K-means\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plot(clean_customers['Detergents_Paper'], \n",
    "     clean_customers['Milk'], \n",
    "     clean_customers['Label'],\n",
    "     ax1)\n",
    "ax1.set_title('K-means: Detergents Paper vs Milk')\n",
    "\n",
    "# DBSCAN\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plot(clean_customers['Detergents_Paper'], \n",
    "     clean_customers['Milk'], \n",
    "     clean_customers['Label_DBSCAN'],\n",
    "     ax2)\n",
    "ax2.set_title('DBSCAN: Detergents Paper vs Milk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Grocery vs Fresh\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# K-means\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plot(clean_customers['Grocery'], \n",
    "     clean_customers['Fresh'], \n",
    "     clean_customers['Label'],\n",
    "     ax1)\n",
    "ax1.set_title('K-means: Grocery vs Fresh')\n",
    "\n",
    "# DBSCAN\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plot(clean_customers['Grocery'], \n",
    "     clean_customers['Fresh'], \n",
    "     clean_customers['Label_DBSCAN'],\n",
    "     ax2)\n",
    "ax2.set_title('DBSCAN: Grocery vs Fresh')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Frozen vs Delicassen\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# K-means\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plot(clean_customers['Frozen'], \n",
    "     clean_customers['Delicassen'], \n",
    "     clean_customers['Label'],\n",
    "     ax1)\n",
    "ax1.set_title('K-means: Frozen vs Delicassen')\n",
    "\n",
    "# DBSCAN\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plot(clean_customers['Frozen'], \n",
    "     clean_customers['Delicassen'], \n",
    "     clean_customers['Label_DBSCAN'],\n",
    "     ax2)\n",
    "ax2.set_title('DBSCAN: Frozen vs Delicassen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medias para K-means\n",
    "print(\"Medias por cluster (K-means):\")\n",
    "print(clean_customers.groupby('Label')[numeric_columns].mean())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Medias para DBSCAN\n",
    "print(\"Medias por cluster (DBSCAN):\")\n",
    "print(clean_customers.groupby('Label_DBSCAN')[numeric_columns].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "**Observaciones sobre el rendimiento de los algoritmos:**\n",
    "\n",
    "1. **K-means parece funcionar mejor para este dataset porque:**\n",
    "   - Crea clusters más balanceados y claramente definidos\n",
    "   - La separación entre grupos es más interpretable desde una perspectiva de negocio\n",
    "   - Los centroides representan bien los patrones de gasto de diferentes tipos de clientes\n",
    "   - La visualización muestra una separación más clara entre los grupos\n",
    "\n",
    "2. **DBSCAN muestra limitaciones en este caso:**\n",
    "   - Clasifica muchos puntos como ruido (-1)\n",
    "   - Los clusters no son tan interpretables desde una perspectiva de negocio\n",
    "   - La densidad variable en los datos hace que sea difícil encontrar parámetros óptimos\n",
    "   - No captura bien la estructura natural de los datos de clientes\n",
    "\n",
    "3. **Razones específicas por las que K-means es mejor:**\n",
    "   - Los datos no tienen formas irregulares que requieran DBSCAN\n",
    "   - Los patrones de gasto tienden a agruparse de manera más esférica\n",
    "   - La interpretación de los centroides es útil para segmentación de clientes\n",
    "   - La varianza dentro de cada cluster es más homogénea\n",
    "\n",
    "4. **Implicaciones para el negocio:**\n",
    "   - K-means proporciona una segmentación más accionable de clientes\n",
    "   - Los grupos son más fáciles de caracterizar y targetear\n",
    "   - Permite una mejor estrategia de marketing segmentado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar diferentes números de clusters (k=2 hasta k=6)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for k in range(2, 7):\n",
    "    # Aplicar K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(customers_scale)\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    sns.scatterplot(\n",
    "        data=clean_customers,\n",
    "        x='Grocery',\n",
    "        y='Fresh',\n",
    "        hue=labels,\n",
    "        ax=axes[k-2],\n",
    "        palette='deep'\n",
    "    )\n",
    "    axes[k-2].set_title(f'K-means con {k} clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular las métricas para cada k\n",
    "for k in range(2, 7):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(customers_scale)\n",
    "    \n",
    "    # Imprimir tamaño de clusters\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\nPara k={k}:\")\n",
    "    print(\"Tamaño de cada cluster:\", dict(zip(unique, counts)))\n",
    "    \n",
    "    # Calcular medias de cada cluster\n",
    "    clean_customers['temp_labels'] = labels\n",
    "    print(\"\\nMedias por cluster:\")\n",
    "    print(clean_customers.groupby('temp_labels')[numeric_columns].mean().round(2))\n",
    "    del clean_customers['temp_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "**Observaciones sobre el número óptimo de clusters:**\n",
    "\n",
    "- k=2 parece ser la mejor opción porque:\n",
    "  * Muestra la separación más clara y natural de los datos\n",
    "  * Los clusters están bien balanceados\n",
    "  * La interpretación es más directa y significativa\n",
    "  * Coincide con lo que vimos en el método del codo\n",
    "\n",
    "- Al aumentar k (3-6):\n",
    "  * Los clusters se vuelven menos definidos\n",
    "  * La separación entre grupos es menos clara\n",
    "  * Se pierde interpretabilidad\n",
    "  * Algunos clusters se vuelven muy pequeños o artificiales\n",
    "\n",
    "**Conclusión:**\n",
    "Para este conjunto de datos específico, k=2 es la mejor opción porque:\n",
    "1. Proporciona la segmentación más robusta y clara\n",
    "2. Es consistente con el método del codo que usamos anteriormente\n",
    "3. Los dos grupos tienen un significado de negocio interpretable\n",
    "4. La división es natural y no forzada como en valores más altos de k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar diferentes combinaciones de eps y min_samples\n",
    "eps_values = [0.3, 0.5, 0.7]\n",
    "min_samples_values = [3, 5, 10]\n",
    "\n",
    "fig, axes = plt.subplots(len(eps_values), len(min_samples_values), figsize=(15, 15))\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        # Aplicar DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(customers_scale)\n",
    "        \n",
    "        # Visualizar resultados\n",
    "        sns.scatterplot(\n",
    "            data=clean_customers,\n",
    "            x='Grocery',\n",
    "            y='Fresh',\n",
    "            hue=labels,\n",
    "            ax=axes[i,j],\n",
    "            palette='deep'\n",
    "        )\n",
    "        axes[i,j].set_title(f'eps={eps}, min_samples={min_samples}')\n",
    "        \n",
    "        # Imprimir métricas\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        print(f'\\neps={eps}, min_samples={min_samples}:')\n",
    "        print(f'Número de clusters: {n_clusters}')\n",
    "        print(f'Número de puntos de ruido: {n_noise}')\n",
    "        print('Tamaño de los clusters:', np.bincount(labels[labels >= 0]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- Los diferentes valores de eps y min_samples afectan significativamente los resultados de DBSCAN:\n",
    "\n",
    "  * Con eps=0.3:\n",
    "    - Se forman clusters más pequeños y compactos\n",
    "    - Hay más puntos clasificados como ruido (-1)\n",
    "    - La segmentación es más granular\n",
    "\n",
    "  * Con eps=0.5:\n",
    "    - Se obtiene un balance entre tamaño de clusters y puntos de ruido\n",
    "    - Los clusters son más grandes que con eps=0.3\n",
    "    - Menor cantidad de puntos clasificados como ruido\n",
    "\n",
    "  * Con eps=0.7:\n",
    "    - Se forman clusters más grandes\n",
    "    - Hay menos puntos clasificados como ruido\n",
    "    - Los clusters tienden a fusionarse\n",
    "\n",
    "  * El parámetro min_samples también influye:\n",
    "    - Valores más bajos (3) resultan en más clusters\n",
    "    - Valores más altos (10) producen menos clusters pero más puntos de ruido\n",
    "    - Afecta la robustez de la detección de clusters\n",
    "\n",
    "- Esta experimentación demuestra la sensibilidad de DBSCAN a sus parámetros, lo cual puede ser una desventaja comparada con K-means para este dataset específico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
